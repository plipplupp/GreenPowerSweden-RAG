{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6e3fb390",
   "metadata": {},
   "source": [
    "## Chunking och Embedding\n",
    "\n",
    "**Syfte:** Att ta alla våra rena `.txt`-filer, dela upp dem i hanterbara \"chunks\", omvandla dessa chunks till vektorer\n",
    "(embeddings) och spara allt i en sökbar vektordatabas.\n",
    "\n",
    "**Körs på:** Google Colab.\n",
    "\n",
    "### Steg 1: Installationer\n",
    "Vi installerar de bibliotek som behövs på Colab-miljön.\n",
    "`sentence-transformers` är för vår lokala embedding-modell.\n",
    "`langchain` har verktygen, och `chromadb` är vår databas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c52d9563",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install langchain-chroma\n",
    "!pip install langchain-huggingface\n",
    "!pip install langchain-text-splitters\n",
    "!pip install sentence-transformers\n",
    "!pip install tqdm\n",
    "!pip install langchain-core\n",
    "\n",
    "# %%\n",
    "import os\n",
    "import shutil\n",
    "from pathlib import Path\n",
    "from tqdm.notebook import tqdm\n",
    "import chromadb\n",
    "import json\n",
    "\n",
    "# Importer från LangChain\n",
    "from langchain_core.documents import Document\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter \n",
    "from langchain_chroma import Chroma \n",
    "from langchain_huggingface import HuggingFaceEmbeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05215890",
   "metadata": {},
   "source": [
    "### Mountar min google drive för att komma åt min zip-fil med alla text-filer som ligger där"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f5dd31a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4166a4b3",
   "metadata": {},
   "source": [
    "### Steg 2: Ladda upp och packa upp data\n",
    "1. I fil-panelen till vänster i Colab, klicka på \"Ladda upp\".\n",
    "2. Välj din lokalt zippade fil: `all_json_files.zip`.\n",
    "3. Vänta tills uppladdningen är klar.\n",
    "4. Kör cellen nedan för att packa upp filerna."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd5534e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----------------------------------------------------------------------\n",
    "# OBS! Vi använder den absoluta sökvägen till filen på Google Drive.\n",
    "# ----------------------------------------------------------------------\n",
    "\n",
    "# Anpassad sökväg baserat på din bild\n",
    "ZIP_FILE_PATH = Path(\"/content/drive/MyDrive/Green_Power_Sweden/all_json_files.zip\")\n",
    "\n",
    "# Detta är namnet på mappen där filerna ska packas upp (i Colabs temporära miljö, /content/)\n",
    "UNZIP_DIR = \"extracted_text\"\n",
    "\n",
    "# Vi använder .exists() på den fullständiga sökvägen till Drive\n",
    "if ZIP_FILE_PATH.exists():\n",
    "    # Hämtar bara filnamnet för utskriften\n",
    "    ZIP_FILE_NAME = ZIP_FILE_PATH.name\n",
    "\n",
    "    print(f\"Hittade {ZIP_FILE_NAME} på Drive, packar upp till {UNZIP_DIR}...\")\n",
    "\n",
    "    # Rensa eventuell gammal data\n",
    "    if Path(UNZIP_DIR).exists():\n",
    "        print(f\"Rensar gammal mapp: {UNZIP_DIR}\")\n",
    "        shutil.rmtree(UNZIP_DIR)\n",
    "\n",
    "    # Använd f-strängar och !unzip\n",
    "    # OBS: Vi måste skicka sökvägen som sträng till shell-kommandot (!unzip)\n",
    "    !unzip -q \"{ZIP_FILE_PATH}\" -d {UNZIP_DIR}\n",
    "    print(f\"\\n--- KLAR! ---\")\n",
    "    print(f\"All text är uppackad till den temporära mappen '{UNZIP_DIR}'\")\n",
    "else:\n",
    "    print(f\"FEL: Kan inte hitta {ZIP_FILE_PATH}. Kontrollera att sökvägen är korrekt och att Drive är monterad.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "374f5a5b",
   "metadata": {},
   "source": [
    "### Steg 3: Definiera sökvägar (på Colab)\n",
    "\n",
    "Sätter upp var vi läser texten från och var vi\n",
    "ska bygga vår nya databas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b29d3be1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mappen där våra uppackade .txt-filer finns\n",
    "TEXT_FILES_DIR = Path(UNZIP_DIR)\n",
    "\n",
    "# Mappen där vi ska spara vår färdiga Chroma-databas\n",
    "DB_PERSIST_DIR = Path(\"/content/drive/MyDrive/Green_Power_Sweden/green_power_sweden_db\")\n",
    "\n",
    "# Rensa eventuell gammal databas\n",
    "if DB_PERSIST_DIR.exists():\n",
    "    shutil.rmtree(DB_PERSIST_DIR)\n",
    "DB_PERSIST_DIR.mkdir()\n",
    "\n",
    "print(f\"Läser textfiler från: {TEXT_FILES_DIR}\")\n",
    "print(f\"Sparar vektordatabas till: {DB_PERSIST_DIR}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d0f8b20",
   "metadata": {},
   "source": [
    "### Steg 4: Ladda alla textdokument\n",
    "\n",
    "Detta är ett kritiskt steg. Vi loopar igenom alla `.txt`-filer,\n",
    "läser innehållet och skapar ett `Document`-objekt.\n",
    "\n",
    "**Viktigast:** Vi sparar originalfilnamnet som `metadata`.\n",
    "Vi rensar också bort `.txt`-suffixet för att få\n",
    "det *riktiga* käll-filnamnet (t.ex. `dom_A_2023.pdf`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c06d04d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_all_texts(directory: Path) -> list[Document]:\n",
    "    \"\"\"Läser alla .json-filer och skapar LangChain-dokument med korrekt metadata.\"\"\"\n",
    "    documents = []\n",
    "    \n",
    "    print(f\"Läser JSON-filer från {directory}...\")\n",
    "    all_files = list(directory.rglob(\"*.json\")) # Vi letar efter JSON nu!\n",
    "    \n",
    "    for file_path in tqdm(all_files, desc=\"Laddar dokument\"):\n",
    "        try:\n",
    "            with open(file_path, 'r', encoding='utf-8') as f:\n",
    "                data = json.load(f)\n",
    "            \n",
    "            # Hämta vår fina metadata\n",
    "            filename = data.get(\"filename\", file_path.name)\n",
    "            full_path = data.get(\"full_path\", \"Okänd sökväg\") # T.ex. \"domar/Kulturmiljö/fil.pdf\"\n",
    "            \n",
    "            # Loopa igenom sidorna i JSON-filen\n",
    "            for page in data.get(\"pages\", []):\n",
    "                page_text = page.get(\"text\", \"\")\n",
    "                page_num = page.get(\"page_number\", 1)\n",
    "                \n",
    "                # Hoppa över tomma sidor\n",
    "                if not page_text.strip():\n",
    "                    continue\n",
    "\n",
    "                # Skapa metadatan för just denna sida\n",
    "                metadata = {\n",
    "                    \"source\": filename,\n",
    "                    \"full_path\": full_path, # Den viktiga sökvägen!\n",
    "                    \"page\": page_num       # Sidnumret!\n",
    "                }\n",
    "                \n",
    "                # Skapa dokumentet\n",
    "                doc = Document(page_content=page_text, metadata=metadata)\n",
    "                documents.append(doc)\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Kunde inte läsa {file_path.name}: {e}\")\n",
    "            \n",
    "    return documents\n",
    "\n",
    "# Ladda alla dokument\n",
    "all_documents = load_all_texts(TEXT_FILES_DIR)\n",
    "print(f\"\\nKlar. Laddade {len(all_documents)} sidor/dokument.\")\n",
    "if all_documents:\n",
    "    print(f\"Exempel metadata: {all_documents[0].metadata}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42b69a3a",
   "metadata": {},
   "source": [
    "### Steg 5: Initiera Chunking-strategi\n",
    "\n",
    "Vi använder `RecursiveCharacterTextSplitter`.\n",
    "`chunk_size` = Max antal tecken per chunk.\n",
    "`chunk_overlap` = Hur många tecken som ska överlappa\n",
    "mellan chunks för att inte tappa kontexten."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67d70299",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=2000,\n",
    "    chunk_overlap=400,\n",
    "    separators=[\"\\n\\n\", \"\\n\", \" \", \"\"] # Ordningen den försöker dela\n",
    ")\n",
    "\n",
    "print(\"Chunkar dokumenten...\")\n",
    "all_chunks = text_splitter.split_documents(all_documents)\n",
    "\n",
    "print(f\"Klar. Totalt antal dokument: {len(all_documents)}\")\n",
    "print(f\"Totalt antal chunks skapade: {len(all_chunks)}\")\n",
    "print(f\"Exempel på en chunks metadata: {all_chunks[0].metadata}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53226eb8",
   "metadata": {},
   "source": [
    "### Steg 6: Initiera Embedding-modellen\n",
    "\n",
    "Vi laddar ner `all-MiniLM-L12-v2` från HuggingFace.\n",
    "Första gången du kör detta kommer Colab att ladda ner\n",
    "modellen (tar ca 1-2 minuter).\n",
    "\n",
    "Vi ställer in den att köras på `cuda` (GPU), vilket gör\n",
    "processen extremt snabb."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "352cf34f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model_name = \"sentence-transformers/all-MiniLM-L12-v2\"\n",
    "model_name = \"sentence-transformers/all-mpnet-base-v2\"\n",
    "model_kwargs = {'device': 'cuda'} # Använd GPU!\n",
    "encode_kwargs = {'normalize_embeddings': False}\n",
    "\n",
    "# print(\"Laddar embedding-modell (all-MiniLM-L12-v2)...\")\n",
    "print(\"Laddar embedding-modell (all-mpnet-base-v2)...\")\n",
    "embedding_model = HuggingFaceEmbeddings(\n",
    "    model_name=model_name,\n",
    "    model_kwargs=model_kwargs,\n",
    "    encode_kwargs=encode_kwargs\n",
    ")\n",
    "print(\"Modell laddad.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9227a620",
   "metadata": {},
   "source": [
    "### Steg 7: Skapa vektordatabasen\n",
    "\n",
    "Nu tar vi alla våra chunks, vår embedding-modell och\n",
    "talar om för ChromaDB att bygga databasen.\n",
    "\n",
    "Detta är det tunga steget. Chroma kommer att:\n",
    "1. Ta en chunk.\n",
    "2. Skicka den till embedding-modellen -> får en vektor.\n",
    "3. Spara vektorn och metadatan i databasen.\n",
    "4. Upprepa för alla tusentals chunks.\n",
    "\n",
    "Detta kan ta 10-30 minuter beroende på antal chunks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a6e9da4",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Bygger vektordatabas... Sparar till {DB_PERSIST_DIR}\")\n",
    "\n",
    "# Detta kommando gör allt: skapar och sparar databasen på disk\n",
    "# Vi använder en tqdm-progress bar för att se hur det går\n",
    "batch_size = 32 # Hur många chunks som ska processas åt gången\n",
    "total_chunks = len(all_chunks)\n",
    "\n",
    "# Vi måste göra det manuellt i en loop för att få en progress bar\n",
    "db = Chroma(\n",
    "    persist_directory=str(DB_PERSIST_DIR),\n",
    "    embedding_function=embedding_model\n",
    ")\n",
    "\n",
    "# Loopa i batcher\n",
    "for i in tqdm(range(0, total_chunks, batch_size), desc=\"Skapar embeddings\"):\n",
    "    batch = all_chunks[i:i + batch_size]\n",
    "    db.add_documents(batch)\n",
    "\n",
    "print(\"\\n--- DATABAS BYGGD! ---\")\n",
    "print(f\"Databasen är sparad i mappen {DB_PERSIST_DIR}.\")\n",
    "print(f\"Totalt antal chunks i databasen: {db._collection.count()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29623b37",
   "metadata": {},
   "source": [
    "### Steg 8: Zippa och ladda ner databasen\n",
    "\n",
    "Nu zippar vi mappen som innehåller vår färdiga databas.\n",
    "Därefter kan du högerklicka på filen `vector_db.zip`\n",
    "i panelen till vänster och välja \"Ladda ned\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29b50e91",
   "metadata": {},
   "outputs": [],
   "source": [
    "ZIP_DB_NAME = \"/content/drive/MyDrive/Green_Power_Sweden/vector_db.zip\"\n",
    "print(f\"Zippar databasmappen {DB_PERSIST_DIR} till {ZIP_DB_NAME}...\")\n",
    "\n",
    "!zip -r {ZIP_DB_NAME} {DB_PERSIST_DIR}\n",
    "\n",
    "print(f\"\\nKlar! Din databas är redo.\")\n",
    "print(f\"Ladda ner filen: {ZIP_DB_NAME} (från panelen till vänster)\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
